# .github/workflows/test_and_fix.yml
name: Test Conversations & Auto-Fix

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger
    inputs:
      fix_mode:
        description: 'Auto-fix mode'
        required: false
        default: 'auto'
        type: choice
        options:
          - auto
          - manual
          - report-only

jobs:
  test-conversations:
    runs-on: ubuntu-latest
    
    env:
      SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
      SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
      SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      WHATSAPP_TOKEN: ${{ secrets.WHATSAPP_TOKEN }}
      WHATSAPP_PHONE_ID: ${{ secrets.WHATSAPP_PHONE_ID }}
      TIMEZONE: 'Africa/Johannesburg'
      TEST_PHONE: '27731863036'
      
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov pytest-html pytest-json-report colorama
    
    - name: Debug - Check environment
      run: |
        echo "=== Checking environment ==="
        echo "Current directory: $(pwd)"
        echo "Directory contents:"
        ls -la
        echo ""
        echo "Python path:"
        python -c "import sys; print('\n'.join(sys.path))"
        echo ""
        echo "Installed packages:"
        pip list | head -20
    
    - name: Setup test environment
      run: |
        echo "Setting up test environment..."
        # Check if setup file exists
        if [ -f "tests/setup_test_env.py" ]; then
          python tests/setup_test_env.py
        else
          echo "Setup file not found, skipping..."
        fi
    
    - name: Run debug test first
      continue-on-error: true
      run: |
        echo "Running debug test to identify issues..."
        # Create a simple inline debug test
        python -c "
        import sys
        import os
        print('Python:', sys.version)
        print('Current dir:', os.getcwd())
        print('Directory contents:', os.listdir('.'))
        print('Checking for services directory:', os.path.exists('services'))
        print('Checking for tests directory:', os.path.exists('tests'))
        
        # Try to import a service
        sys.path.insert(0, '.')
        try:
            import services.refiloe
            print('âœ… Can import services.refiloe')
        except ImportError as e:
            print(f'âŒ Cannot import services.refiloe: {e}')
        except Exception as e:
            print(f'âŒ Error: {type(e).__name__}: {e}')
        "
        
        # If debug test file exists, run it too
        if [ -f "tests/test_debug.py" ]; then
          echo "Running test_debug.py..."
          python tests/test_debug.py || true
        fi
    
    - name: Run conversation flow tests
      id: run-tests
      continue-on-error: true
      run: |
        echo "Running conversation flow tests..."
        
        # Create default results in case tests fail to run
        echo '{"summary": {"total": 0, "passed": 0, "failed": 0}, "tests": []}' > test-results.json
        echo "<html><body>Tests not run</body></html>" > test-report.html
        
        # Try to run the tests and capture the output
        if python -m pytest tests/test_conversation_flows.py \
          --html=test-report.html \
          --self-contained-html \
          --json-report \
          --json-report-file=test-results.json \
          -v \
          --tb=short \
          --capture=no 2>&1 | tee test-output.log; then
          echo "âœ… Tests passed!"
          echo "test_status=success" >> $GITHUB_ENV
        else
          EXIT_CODE=$?
          echo "âŒ Tests failed with exit code $EXIT_CODE"
          echo "test_status=failure" >> $GITHUB_ENV
          
          # Show the last 50 lines of output for debugging
          echo "=== Last 50 lines of test output ==="
          tail -50 test-output.log || true
          
          # Check if it's an import error
          if grep -q "ImportError\|ModuleNotFoundError" test-output.log; then
            echo "âš ï¸ Import errors detected - missing modules or __init__.py files"
          fi
        fi
        
        # Always show test summary if json exists
        if [ -f "test-results.json" ]; then
          echo "=== Test Summary ==="
          python -c "import json; data=json.load(open('test-results.json')); print(f\"Total: {data.get('summary', {}).get('total', 0)}, Passed: {data.get('summary', {}).get('passed', 0)}, Failed: {data.get('summary', {}).get('failed', 0)}\")" || true
        fi
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results
        path: |
          test-report.html
          test-results.json
          test-output.log
    
    - name: Display test results
      if: always()
      run: |
        echo "=== Displaying Test Results ==="
        
        # Show JSON results if available
        if [ -f "test-results.json" ]; then
          echo "Test Results JSON:"
          cat test-results.json | python -m json.tool | head -100 || true
          
          # Extract and show failed tests
          echo ""
          echo "=== Failed Tests ==="
          python -c "
        import json
        try:
            with open('test-results.json') as f:
                data = json.load(f)
                for test in data.get('tests', []):
                    if test.get('outcome') == 'failed':
                        print(f\"âŒ {test.get('nodeid', 'Unknown test')}:\")
                        print(f\"   {test.get('call', {}).get('longrepr', 'No error details')[:200]}\")
                        print()
        except Exception as e:
            print(f'Could not parse results: {e}')
          " || true
        else
          echo "No test results JSON found"
        fi
    
    - name: Analyze failures and generate fixes
      id: analyze-fixes
      if: steps.run-tests.outcome == 'failure'
      run: |
        echo "Analyzing test failures..."
        python tests/auto_fix_generator.py
    
    - name: Apply auto-fixes and create PR
      if: steps.analyze-fixes.outcome == 'success' && github.event.inputs.fix_mode != 'report-only'
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      run: |
        # Configure git
        git config --global user.name 'Refiloe Auto-Fix Bot'
        git config --global user.email 'bot@refiloe.ai'
        
        # Create fix branch
        BRANCH_NAME="auto-fix-$(date +%Y%m%d-%H%M%S)"
        git checkout -b $BRANCH_NAME
        
        # Apply fixes
        python tests/apply_fixes.py
        
        # Commit changes if any
        if [[ -n $(git status -s) ]]; then
          git add -A
          git commit -m "ðŸ”§ Auto-fix: Resolve conversation flow issues
          
          Automated fixes for test failures:
          $(python tests/get_fix_summary.py)"
          
          # Push branch
          git push origin $BRANCH_NAME
          
          # Create PR
          gh pr create \
            --title "ðŸ”§ Auto-Fix: Conversation Flow Issues" \
            --body "$(python tests/generate_pr_description.py)" \
            --base main \
            --head $BRANCH_NAME \
            --label "auto-fix" \
            --assignee ${{ github.actor }}
        else
          echo "No fixes needed or could be applied automatically"
        fi
    
    - name: Post test summary to PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const results = JSON.parse(fs.readFileSync('test-results.json', 'utf8'));
          
          const passed = results.tests.filter(t => t.outcome === 'passed').length;
          const failed = results.tests.filter(t => t.outcome === 'failed').length;
          const total = results.tests.length;
          
          const emoji = failed === 0 ? 'âœ…' : 'âŒ';
          const status = failed === 0 ? 'All tests passed!' : `${failed} test(s) failed`;
          
          const comment = `## ${emoji} Conversation Flow Test Results
          
          **Status:** ${status}
          **Passed:** ${passed}/${total}
          
          ${failed > 0 ? '### Failed Tests:\n' + results.tests
            .filter(t => t.outcome === 'failed')
            .map(t => `- âŒ ${t.nodeid}: ${t.call.longrepr}`)
            .join('\n') : ''}
          
          [View detailed report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          
          ${failed > 0 && '${{ steps.analyze-fixes.outcome }}' === 'success' ? 
            'ðŸ”§ **Auto-fixes available!** A PR has been created with suggested fixes.' : ''}`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
    
    - name: Cleanup test data
      if: always()
      run: |
        echo "Cleaning up test data from Supabase..."
        python tests/cleanup_test_data.py
    
    - name: Send notification
      if: failure() && github.ref == 'refs/heads/main'
      run: |
        echo "Tests failed on main branch - would send notification here"
        # Could integrate with Slack, email, or WhatsApp notification
